{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff27e80a",
   "metadata": {},
   "source": [
    "# 1. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Class Definition\n",
    "\"\"\"\n",
    "# BigQuery Connector Utility Class\n",
    "\n",
    "This notebook contains the main BigQueryConnector class.\n",
    "Import this notebook into other notebooks using: %run utils/bigquery_connector.ipynb\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de41adc",
   "metadata": {},
   "source": [
    "## 1.1 Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from typing import List, Optional\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import inspect\n",
    "import types\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a73c74",
   "metadata": {},
   "source": [
    "## 1.2 Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Main Class\n",
    "class BigQueryConnector:\n",
    "    \"\"\"\n",
    "    Professional BigQuery Connector Class for Jupyter Notebooks\n",
    "    \n",
    "    Features:\n",
    "    - Easy connection setup\n",
    "    - Interactive data exploration\n",
    "    - Query validation and execution\n",
    "    - DataFrame integration\n",
    "    - Notebook-friendly outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize BigQuery client with environment variables\"\"\"\n",
    "        try:\n",
    "            # Setup credentials\n",
    "            credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "            if credentials_path:\n",
    "                os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "                print(f\"‚úì Credentials loaded from: {credentials_path}\")\n",
    "            \n",
    "            # Initialize client\n",
    "            self.project_id = os.getenv('PROJECT_ID')\n",
    "            self.dataset_id = os.getenv('DATASET_ID')\n",
    "            \n",
    "            if not self.project_id:\n",
    "                raise ValueError(\"PROJECT_ID not found in environment variables\")\n",
    "            \n",
    "            self.client = bigquery.Client(project=self.project_id)\n",
    "            print(f\"‚úì BigQuery client initialized for project: {self.project_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Initialization failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def test_connection(self) -> bool:\n",
    "        \"\"\"Test BigQuery connection with simple query\"\"\"\n",
    "        try:\n",
    "            query = \"SELECT 1 as test, CURRENT_DATETIME() as timestamp\"\n",
    "            result = self.client.query(query).to_dataframe()\n",
    "            print(\"‚úì Connection successful!\")\n",
    "            print(f\"üìÖ Server time: {result.iloc[0]['timestamp']}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_tables(self) -> List[str]:\n",
    "        \"\"\"List all tables in the configured dataset\"\"\"\n",
    "        try:\n",
    "            if not self.dataset_id:\n",
    "                print(\"‚ö†Ô∏è DATASET_ID not configured. Use list_datasets() to see available datasets.\")\n",
    "                return []\n",
    "            \n",
    "            dataset_ref = self.client.dataset(self.dataset_id)\n",
    "            tables = list(self.client.list_tables(dataset_ref))\n",
    "            table_names = [table.table_id for table in tables]\n",
    "            \n",
    "            print(f\"üìä Dataset: {self.dataset_id}\")\n",
    "            print(f\"üìã Available tables ({len(table_names)}):\")\n",
    "            for i, table in enumerate(table_names, 1):\n",
    "                print(f\"  {i:2d}. {table}\")\n",
    "            \n",
    "            return table_names\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to list tables: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def query_data(self, query: str, dry_run: bool = False) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Execute query and return DataFrame\n",
    "        \n",
    "        Args:\n",
    "            query (str): SQL query to execute\n",
    "            dry_run (bool): If True, only validate query without running\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if dry_run:\n",
    "                job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "                job = self.client.query(query, job_config=job_config)\n",
    "                print(f\"‚úì Query validation successful\")\n",
    "                print(f\"üìä Estimated bytes processed: {job.total_bytes_processed:,}\")\n",
    "                print(f\"üí∞ Estimated cost: ${(job.total_bytes_processed / 1024**4) * 5:.4f}\")\n",
    "                return None\n",
    "            \n",
    "            print(\"üîÑ Executing query...\")\n",
    "            df = self.client.query(query).to_dataframe()\n",
    "            print(f\"‚úì Query executed successfully\")\n",
    "            print(f\"üìä Rows returned: {len(df):,}\")\n",
    "            print(f\"üìã Columns: {len(df.columns)}\")\n",
    "            print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Query failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_table_info(self, table_name: str) -> Optional[bigquery.Table]:\n",
    "        \"\"\"Get comprehensive table information including schema and sample data\"\"\"\n",
    "        try:\n",
    "            if not self.dataset_id:\n",
    "                print(\"‚ö†Ô∏è DATASET_ID not configured\")\n",
    "                return None\n",
    "                \n",
    "            table_ref = self.client.dataset(self.dataset_id).table(table_name)\n",
    "            table = self.client.get_table(table_ref)\n",
    "            \n",
    "            # Basic table info\n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(f\"üìä TABLE INFORMATION: {table_name}\")\n",
    "            print(f\"=\"*60)\n",
    "            print(f\"üìà Total Rows: {table.num_rows:,}\")\n",
    "            print(f\"üíæ Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
    "            print(f\"üìÖ Created: {table.created}\")\n",
    "            print(f\"üîÑ Modified: {table.modified}\")\n",
    "            \n",
    "            # Description if available\n",
    "            if table.description:\n",
    "                print(f\"üìù Description: {table.description}\")\n",
    "            \n",
    "            # Schema information\n",
    "            print(f\"\\nüìã SCHEMA ({len(table.schema)} columns):\")\n",
    "            print(f\"{'No.':<4} {'Column Name':<25} {'Data Type':<15} {'Mode':<10} {'Description':<30}\")\n",
    "            print(\"-\" * 90)\n",
    "            \n",
    "            for i, field in enumerate(table.schema, 1):\n",
    "                mode = field.mode if field.mode else \"NULLABLE\"\n",
    "                description = field.description[:30] if field.description else \"\"\n",
    "                print(f\"{i:<4} {field.name:<25} {field.field_type:<15} {mode:<10} {description:<30}\")\n",
    "            \n",
    "            # Sample data\n",
    "            print(f\"\\nüîç SAMPLE DATA (first 5 rows):\")\n",
    "            sample_df = self.quick_query(table_name, limit=5)\n",
    "            if sample_df is not None and not sample_df.empty:\n",
    "                # Display with better formatting\n",
    "                display(sample_df)\n",
    "            \n",
    "            return table\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to get table info: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def list_datasets(self) -> List[str]:\n",
    "        \"\"\"List all datasets in the project\"\"\"\n",
    "        try:\n",
    "            datasets = list(self.client.list_datasets())\n",
    "            \n",
    "            print(f\"üìÅ PROJECT: {self.project_id}\")\n",
    "            print(f\"üìÇ Available datasets ({len(datasets)}):\")\n",
    "            \n",
    "            for i, dataset in enumerate(datasets, 1):\n",
    "                # Get dataset info\n",
    "                dataset_obj = self.client.get_dataset(dataset.dataset_id)\n",
    "                tables_count = len(list(self.client.list_tables(dataset_obj)))\n",
    "                \n",
    "                print(f\"  {i:2d}. {dataset.dataset_id:<30} ({tables_count} tables)\")\n",
    "                \n",
    "            return [dataset.dataset_id for dataset in datasets]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to list datasets: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def quick_query(self, table_name: str, limit: int = 10) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Quick query to preview table data\"\"\"\n",
    "        if not self.dataset_id:\n",
    "            print(\"‚ö†Ô∏è DATASET_ID not configured\")\n",
    "            return None\n",
    "            \n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
    "            LIMIT {limit}\n",
    "        \"\"\"\n",
    "        return self.query_data(query)\n",
    "    \n",
    "    def get_table_schema(self, table_name: str) -> Optional[List[dict]]:\n",
    "        \"\"\"Get table schema as a list of dictionaries\"\"\"\n",
    "        try:\n",
    "            if not self.dataset_id:\n",
    "                print(\"‚ö†Ô∏è DATASET_ID not configured\")\n",
    "                return None\n",
    "                \n",
    "            table_ref = self.client.dataset(self.dataset_id).table(table_name)\n",
    "            table = self.client.get_table(table_ref)\n",
    "            \n",
    "            schema_list = []\n",
    "            for field in table.schema:\n",
    "                schema_list.append({\n",
    "                    'name': field.name,\n",
    "                    'type': field.field_type,\n",
    "                    'mode': field.mode or 'NULLABLE',\n",
    "                    'description': field.description or ''\n",
    "                })\n",
    "            \n",
    "            return schema_list\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to get schema: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def count_rows(self, table_name: str, where_clause: str = \"\") -> Optional[int]:\n",
    "        \"\"\"Count rows in a table with optional WHERE clause\"\"\"\n",
    "        try:\n",
    "            if not self.dataset_id:\n",
    "                print(\"‚ö†Ô∏è DATASET_ID not configured\")\n",
    "                return None\n",
    "            \n",
    "            where_part = f\"WHERE {where_clause}\" if where_clause else \"\"\n",
    "            query = f\"\"\"\n",
    "                SELECT COUNT(*) as row_count\n",
    "                FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
    "                {where_part}\n",
    "            \"\"\"\n",
    "            \n",
    "            result = self.query_data(query)\n",
    "            if result is not None:\n",
    "                count = result.iloc[0]['row_count']\n",
    "                print(f\"üìä Row count for {table_name}: {count:,}\")\n",
    "                return count\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to count rows: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_column_stats(self, table_name: str, column_name: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Get basic statistics for a specific column\"\"\"\n",
    "        try:\n",
    "            if not self.dataset_id:\n",
    "                print(\"‚ö†Ô∏è DATASET_ID not configured\")\n",
    "                return None\n",
    "            \n",
    "            query = f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_count,\n",
    "                    COUNT({column_name}) as non_null_count,\n",
    "                    COUNT(DISTINCT {column_name}) as unique_count,\n",
    "                    MIN({column_name}) as min_value,\n",
    "                    MAX({column_name}) as max_value\n",
    "                FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
    "            \"\"\"\n",
    "            \n",
    "            stats = self.query_data(query)\n",
    "            if stats is not None:\n",
    "                print(f\"üìä Statistics for column '{column_name}' in table '{table_name}':\")\n",
    "                display(stats)\n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to get column stats: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def search_tables(self, search_term: str) -> List[str]:\n",
    "        \"\"\"Search for tables containing the search term in their name\"\"\"\n",
    "        try:\n",
    "            if not self.dataset_id:\n",
    "                print(\"‚ö†Ô∏è DATASET_ID not configured\")\n",
    "                return []\n",
    "            \n",
    "            all_tables = self.get_tables()\n",
    "            matching_tables = [table for table in all_tables if search_term.lower() in table.lower()]\n",
    "            \n",
    "            print(f\"üîç Tables matching '{search_term}':\")\n",
    "            for i, table in enumerate(matching_tables, 1):\n",
    "                print(f\"  {i}. {table}\")\n",
    "            \n",
    "            return matching_tables\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Search failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def export_table_to_csv(self, table_name: str, filename: str = None, limit: int = None) -> str:\n",
    "        \"\"\"Export table data to CSV file\"\"\"\n",
    "        try:\n",
    "            if not self.dataset_id:\n",
    "                print(\"‚ö†Ô∏è DATASET_ID not configured\")\n",
    "                return None\n",
    "            \n",
    "            # Prepare query\n",
    "            limit_clause = f\"LIMIT {limit}\" if limit else \"\"\n",
    "            query = f\"\"\"\n",
    "                SELECT *\n",
    "                FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n",
    "                {limit_clause}\n",
    "            \"\"\"\n",
    "            \n",
    "            # Get data\n",
    "            df = self.query_data(query)\n",
    "            if df is None:\n",
    "                return None\n",
    "            \n",
    "            # Prepare filename\n",
    "            if not filename:\n",
    "                filename = f\"{table_name}_export\"\n",
    "            \n",
    "            # Save to CSV\n",
    "            save_query_result(df, filename, format='csv')\n",
    "            return f\"../outputs/exports/{filename}.csv\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Export failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def validate_query(self, query: str) -> bool:\n",
    "        \"\"\"Validate SQL query syntax without executing\"\"\"\n",
    "        return self.query_data(query, dry_run=True) is not None\n",
    "    \n",
    "    def select_table_by_index(self, index: int) -> Optional[str]:\n",
    "        \"\"\"Select table by index (0-based)\"\"\"\n",
    "        try:\n",
    "            tables = self.get_tables()\n",
    "            if 0 <= index < len(tables):\n",
    "                selected_table = tables[index]\n",
    "                print(f\"‚úÖ Selected table at index {index}: {selected_table}\")\n",
    "                return selected_table\n",
    "            else:\n",
    "                print(f\"‚ùå Index {index} out of range. Available: 0-{len(tables)-1}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to select table: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def select_table_by_number(self, number: int) -> Optional[str]:\n",
    "        \"\"\"Select table by display number (1-based)\"\"\"\n",
    "        try:\n",
    "            tables = self.get_tables()\n",
    "            if 1 <= number <= len(tables):\n",
    "                selected_table = tables[number - 1]  # Convert to 0-based index\n",
    "                print(f\"‚úÖ Selected table #{number}: {selected_table}\")\n",
    "                return selected_table\n",
    "            else:\n",
    "                print(f\"‚ùå Invalid number. Choose between 1-{len(tables)}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to select table: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def explore_table_by_index(self, index: int):\n",
    "        \"\"\"Select and explore table by index in one go\"\"\"\n",
    "        selected_table = self.select_table_by_index(index)\n",
    "        if selected_table:\n",
    "            print(f\"\\nüîç Exploring table: {selected_table}\")\n",
    "            self.get_table_info(selected_table)\n",
    "            sample_df = self.quick_query(selected_table, limit=5)\n",
    "            if sample_df is not None:\n",
    "                display(sample_df)\n",
    "        return selected_table\n",
    "    \n",
    "    def extract_table_data(self, table_name: str, \n",
    "                          columns: list = None, \n",
    "                          where_clause: str = None, \n",
    "                          order_by: str = None, \n",
    "                          limit: int = None,\n",
    "                          sample_percent: float = None) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract data from a single table with flexible options\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): Name of the table to extract from\n",
    "            columns (list): List of columns to select (default: all columns)\n",
    "            where_clause (str): WHERE condition (without 'WHERE' keyword)\n",
    "            order_by (str): ORDER BY clause (without 'ORDER BY' keyword)\n",
    "            limit (int): Maximum number of rows to return\n",
    "            sample_percent (float): Percentage of data to sample (0.1 = 10%)\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Extracted data or None if failed\n",
    "            \n",
    "        Examples:\n",
    "            # Extract all data\n",
    "            df = bq.extract_table_data('customers')\n",
    "            \n",
    "            # Extract specific columns\n",
    "            df = bq.extract_table_data('customers', columns=['id', 'name', 'email'])\n",
    "            \n",
    "            # Extract with conditions\n",
    "            df = bq.extract_table_data('orders', \n",
    "                                     where_clause=\"status = 'completed' AND amount > 100\",\n",
    "                                     order_by=\"created_at DESC\",\n",
    "                                     limit=1000)\n",
    "            \n",
    "            # Extract sample data\n",
    "            df = bq.extract_table_data('large_table', sample_percent=1.0)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.dataset_id:\n",
    "                print(\"‚ö†Ô∏è DATASET_ID not configured\")\n",
    "                return None\n",
    "            \n",
    "            # Build SELECT clause\n",
    "            if columns:\n",
    "                # Validate column names (simple check)\n",
    "                clean_columns = [col.strip() for col in columns]\n",
    "                select_clause = \", \".join(clean_columns)\n",
    "            else:\n",
    "                select_clause = \"*\"\n",
    "            \n",
    "            # Build FROM clause\n",
    "            from_clause = f\"`{self.project_id}.{self.dataset_id}.{table_name}`\"\n",
    "            \n",
    "            # Add TABLESAMPLE if sample_percent provided\n",
    "            if sample_percent:\n",
    "                if 0 < sample_percent <= 100:\n",
    "                    from_clause += f\" TABLESAMPLE SYSTEM ({sample_percent} PERCENT)\"\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è sample_percent must be between 0 and 100\")\n",
    "                    return None\n",
    "            \n",
    "            # Build WHERE clause\n",
    "            where_part = f\"WHERE {where_clause}\" if where_clause else \"\"\n",
    "            \n",
    "            # Build ORDER BY clause\n",
    "            order_part = f\"ORDER BY {order_by}\" if order_by else \"\"\n",
    "            \n",
    "            # Build LIMIT clause\n",
    "            limit_part = f\"LIMIT {limit}\" if limit else \"\"\n",
    "            \n",
    "            # Construct final query\n",
    "            query = f\"\"\"\n",
    "                SELECT {select_clause}\n",
    "                FROM {from_clause}\n",
    "                {where_part}\n",
    "                {order_part}\n",
    "                {limit_part}\n",
    "            \"\"\".strip()\n",
    "            \n",
    "            print(f\"üîÑ Extracting data from table: {table_name}\")\n",
    "            if where_clause:\n",
    "                print(f\"üìã Filter: {where_clause}\")\n",
    "            if columns:\n",
    "                print(f\"üìä Columns: {len(columns)} selected\")\n",
    "            if limit:\n",
    "                print(f\"üìè Limit: {limit:,} rows\")\n",
    "            if sample_percent:\n",
    "                print(f\"üé≤ Sample: {sample_percent}%\")\n",
    "            \n",
    "            # Execute query\n",
    "            df = self.query_data(query)\n",
    "            \n",
    "            if df is not None:\n",
    "                print(f\"‚úÖ Successfully extracted {len(df):,} rows from {table_name}\")\n",
    "                \n",
    "                # Show column info if specific columns selected\n",
    "                if columns and len(df) > 0:\n",
    "                    print(f\"üìã Extracted columns: {list(df.columns)}\")\n",
    "                \n",
    "                return df\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to extract data from {table_name}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error extracting data from {table_name}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afbcc4",
   "metadata": {},
   "source": [
    "## 1.3 Helper Functions for Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9baf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Helper Functions for Notebooks\n",
    "def display_query_results(df: pd.DataFrame, title: str = \"Query Results\") -> pd.DataFrame:\n",
    "    \"\"\"Display query results in notebook-friendly format\"\"\"\n",
    "    print(f\"\\nüìä {title}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üìà Rows: {len(df):,} | Columns: {len(df.columns)}\")\n",
    "    print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"üìã Column types: {dict(df.dtypes)}\")\n",
    "    print(\"\\nüîç Sample data:\")\n",
    "    \n",
    "    # Display with pandas styling for better visualization\n",
    "    if len(df) > 0:\n",
    "        display(df.head(10))\n",
    "    else:\n",
    "        print(\"No data to display\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_query_result(df: pd.DataFrame, filename: str, format: str = 'csv') -> str:\n",
    "    \"\"\"Save query results to file\"\"\"\n",
    "    output_dir = '../outputs/exports'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    filepath = f\"{output_dir}/{filename}.{format}\"\n",
    "    \n",
    "    try:\n",
    "        if format == 'csv':\n",
    "            df.to_csv(filepath, index=False)\n",
    "        elif format == 'parquet':\n",
    "            df.to_parquet(filepath, index=False)\n",
    "        elif format == 'excel':\n",
    "            df.to_excel(filepath, index=False)\n",
    "        elif format == 'json':\n",
    "            df.to_json(filepath, orient='records', indent=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "        \n",
    "        print(f\"üíæ Data saved to: {filepath}\")\n",
    "        print(f\"üìä Saved {len(df):,} rows and {len(df.columns)} columns\")\n",
    "        return filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed to save file: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_sample_queries(bq_connector) -> dict:\n",
    "    \"\"\"Generate sample queries for exploration\"\"\"\n",
    "    if not bq_connector.dataset_id:\n",
    "        print(\"‚ö†Ô∏è DATASET_ID not configured\")\n",
    "        return {}\n",
    "    \n",
    "    tables = bq_connector.get_tables()\n",
    "    if not tables:\n",
    "        print(\"‚ö†Ô∏è No tables found\")\n",
    "        return {}\n",
    "    \n",
    "    first_table = tables[0]\n",
    "    project_id = bq_connector.project_id\n",
    "    dataset_id = bq_connector.dataset_id\n",
    "    \n",
    "    sample_queries = {\n",
    "        \"basic_select\": f\"SELECT * FROM `{project_id}.{dataset_id}.{first_table}` LIMIT 10\",\n",
    "        \"count_rows\": f\"SELECT COUNT(*) as total_rows FROM `{project_id}.{dataset_id}.{first_table}`\",\n",
    "        \"column_info\": f\"\"\"\n",
    "            SELECT \n",
    "                column_name,\n",
    "                data_type,\n",
    "                is_nullable\n",
    "            FROM `{project_id}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`\n",
    "            WHERE table_name = '{first_table}'\n",
    "        \"\"\",\n",
    "        \"distinct_count\": f\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_rows,\n",
    "                COUNT(DISTINCT *) as unique_rows\n",
    "            FROM `{project_id}.{dataset_id}.{first_table}`\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    print(\"üìù Sample queries generated:\")\n",
    "    for name, query in sample_queries.items():\n",
    "        print(f\"  - {name}\")\n",
    "    \n",
    "    return sample_queries\n",
    "\n",
    "def quick_data_profile(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Quick data profiling for any DataFrame\"\"\"\n",
    "    print(\"üìä QUICK DATA PROFILE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"üìà Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"üíæ Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\nüö® Missing values:\")\n",
    "        for col, count in missing[missing > 0].items():\n",
    "            print(f\"  {col}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\n‚úì No missing values\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nüìã Data types:\")\n",
    "    type_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in type_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Numeric columns summary\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nüìä Numeric columns summary:\")\n",
    "        display(df[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5897f5",
   "metadata": {},
   "source": [
    "## 1.4 Utility Functions for BigQuery Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5296b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Utility Functions for BigQuery Operations\n",
    "def setup_environment_check():\n",
    "    \"\"\"Check if environment is properly configured\"\"\"\n",
    "    print(\"üîç Environment Configuration Check\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check environment variables\n",
    "    env_vars = ['PROJECT_ID', 'DATASET_ID', 'GOOGLE_APPLICATION_CREDENTIALS']\n",
    "    for var in env_vars:\n",
    "        value = os.getenv(var)\n",
    "        status = \"‚úì Set\" if value else \"‚ùå Missing\"\n",
    "        print(f\"{var}: {status}\")\n",
    "        if value and var == 'GOOGLE_APPLICATION_CREDENTIALS':\n",
    "            file_exists = os.path.exists(value)\n",
    "            print(f\"  File exists: {'‚úì Yes' if file_exists else '‚ùå No'}\")\n",
    "    \n",
    "    print(\"\\nüì¶ Required packages:\")\n",
    "    required_packages = ['google-cloud-bigquery', 'pandas', 'IPython']\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "            print(f\"  {package}: ‚úì Installed\")\n",
    "        except ImportError:\n",
    "            print(f\"  {package}: ‚ùå Missing\")\n",
    "\n",
    "def format_sql_query(query: str) -> str:\n",
    "    \"\"\"Basic SQL formatting for better readability\"\"\"\n",
    "    # Simple formatting - can be enhanced with sqlparse\n",
    "    formatted = query.strip()\n",
    "    formatted = formatted.replace(',', ',\\n    ')\n",
    "    formatted = formatted.replace(' FROM ', '\\nFROM ')\n",
    "    formatted = formatted.replace(' WHERE ', '\\nWHERE ')\n",
    "    formatted = formatted.replace(' GROUP BY ', '\\nGROUP BY ')\n",
    "    formatted = formatted.replace(' ORDER BY ', '\\nORDER BY ')\n",
    "    formatted = formatted.replace(' HAVING ', '\\nHAVING ')\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "print(\"‚úì BigQueryConnector class and helper functions loaded!\")\n",
    "print(\"‚úì Ready to use: bq = BigQueryConnector()\")\n",
    "print(\"‚úì Available helper functions: display_query_results, save_query_result, quick_data_profile\")\n",
    "print(\"‚úì Utility functions: setup_environment_check, create_sample_queries, format_sql_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7305640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ METHOD 2: Filter only custom methods\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def get_custom_methods(obj_or_class):\n",
    "    \"\"\"Get only custom methods (exclude Python internal methods)\"\"\"\n",
    "    methods = []\n",
    "    for name in dir(obj_or_class):\n",
    "        # Skip internal Python methods (yang dimulai dan diakhiri __)\n",
    "        if not name.startswith('_') or name in ['__init__']:\n",
    "            attr = getattr(obj_or_class, name)\n",
    "            if callable(attr):  # Check if it's a method/function\n",
    "                methods.append(name)\n",
    "    return methods\n",
    "\n",
    "# Dari class\n",
    "custom_methods = get_custom_methods(BigQueryConnector)\n",
    "print(\"Custom methods in BigQueryConnector:\")\n",
    "for i, method in enumerate(custom_methods, 1):\n",
    "    print(f\"  {i:2d}. {method}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
